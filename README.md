# ğŸ“Š Excel-AI Engine â€” LLM Powered Excel Data Intelligence System

### âœ¨ Natural-Language Data Analysis | Excel Automation | AI-Driven Insights

> Upload any Excel file â†’ Ask questions in English â†’ Get SQL-like results, summaries, pivots, filters, joins & insights powered by LLMs.

---

## ğŸš€ Key Features

| Capability | Details |
|---|---|
ğŸ§  LLM-Powered Query Understanding | Convert natural language into structured data ops  
ğŸ“ Excel File Upload | Works with any `.xlsx` file  
ğŸ“Š Structured Data Analysis | Filters, aggregations, joins, pivots, math ops  
ğŸ“… Date Operations | Extract year/month/day, time diff  
ğŸ—£ï¸ Optional Text Intelligence | Summaries, sentiment (LLM-based)  
ğŸ–§ REST APIs | `/upload`, `/query/run`  
âš™ï¸ Local AI | Works fully offline via **Ollama + LLaMA3**  
ğŸ’¡ Auto Sample Excel Generator | 1000+ rows structured and unstructured data 

---


## ğŸ§  Tech Stack

| Component | Tool |
|---|---|
Language | Python 3.10+
Backend | FastAPI
Compute | Pandas, OpenPyXL
LLM | Ollama â€” LLaMA3
Runtime | Uvicorn
Mode | CLI + REST API

---

## ğŸ— System Architecture

```mermaid
flowchart TD
    A[User Request] --> B[FastAPI Server]
    B --> C[LLM - Ollama LLaMA3]
    C -->|JSON plan| D[Query Planner]
    D --> E[Pandas Engine]
    E --> F[Excel I/O]
    F --> G[Final Response]

```
##ğŸ“ Folder Structure
## ğŸ“ Folder Structure

```text
excel-ai-engine/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # FastAPI entry point
â”‚   â”œâ”€â”€ router.py              # API routing
â”‚   â”œâ”€â”€ llm_agent/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ llm_agent.py       # LLM interface (Ollama / OpenAI / etc.)
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ orchestrator.py    # NL query â†’ operation planner
â”‚       â””â”€â”€ excel_processor.py # Pandas Excel operations
â”œâ”€â”€ data/                      # Generated Excel files stored here
â”œâ”€â”€ cli_orchestrator.py        # CLI mode for running natural queries
â””â”€â”€ README.md                  # Documentation
## ğŸš€ Quick Start
```
### 1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/YOUR_USERNAME/excel-ai-engine.git
cd excel-ai-engine

```
### 2ï¸âƒ£ Create virtual environment
```python -m venv venv
source venv/bin/activate      # macOS/Linux
venv\Scripts\activate         # Windows
```
### 3ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```
### 4ï¸âƒ£ Install & pull LLaMA3 model (Ollama)
Download Ollama ğŸ‘‰ https://ollama.com/download
Then pull the model:
```
ollama pull llama3
```
### 5ï¸âƒ£ Run the server
```
uvicorn app.main:app --reload
```
## ğŸ§ª Roadmap

- [x] âœ… **Excel + Pandas + LLM agent**  
  - Backend: FastAPI + Pandas, LLM integration via Ollama.
  - Key file(s): `app/services/orchestrator.py`, `app/llm_agent/llm_agent.py`

- [x] âœ… **CLI + API**  
  - CLI runner: `cli_orchestrator.py`  
  - API: `POST /upload/file`, `POST /query/run` (see `app/router.py`)

- [ ] â³ **Streamlit UI dashboard** â€” *quick starter*
  - File: `ui.py` (drop in repo root)
  - Run: `streamlit run ui.py`

```python
# ui.py (Streamlit quick dashboard)
import streamlit as st
import requests
import tempfile

st.title("Excel-AI Engine â€” Demo UI")

uploaded = st.file_uploader("Upload Excel (.xlsx)", type=["xlsx"])
query = st.text_input("Ask a question about your data")

if st.button("Upload & Run") and uploaded:
    # save to data/
    path = f"data/{uploaded.name}"
    with open(path, "wb") as f:
        f.write(uploaded.getbuffer())
    st.success(f"Saved to {path}")

    payload = {"file_path": path, "sheet_name": "Structured", "query": query}
    resp = requests.post("http://localhost:8000/query/run", json=payload)
    st.subheader("Result")
    try:
        st.json(resp.json())
    except Exception:
        st.write(resp.text)

```
## ğŸ³ Docker Support

### ğŸ“¦ Dockerfile

```dockerfile
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy dependencies file
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy project code
COPY . .

# Expose backend port
EXPOSE 8000

# Run FastAPI server
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```
